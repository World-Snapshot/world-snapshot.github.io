## Summary

In this article, we attempt to address the real challenges currently faced by Computer Vision:

- **How do we define and implement the ideal world model?** Our answer: (1) 9-point definitions; (2) Wave Generative method (abbreviated as WaveGen); (3) World Snapshot methodology: "Encoding all information onto boundaries + Conformal symmetry" — has a similar meaning to LLMs' "Predict next token" methodology.
- **How do we achieve true multimodal fusion and storage?** Our answer: Store maximum information within images themselves, achieving near 100% or theoretically exceeding 100% modal fusion degree. This is training the world itself.
- **How do we make AI understand physics?** Our answer: Transform the generative model's parameter space into an engine-like space. We analyze error sources and approach perfection through multiple angles—training strategies, high-dimensional spaces, and low-dimensional spaces, i.e., via least action principle + core space + WSG format, correspondingly.
- **How do we achieve causal generation?** Our answer: Recognize and make world divides into internal and external flow velocity differences (core space & pixel world). Humans control the slower internal world, thus achieving causal reality generation. To be more specific, our core space acts like a operating micro-causal space/engine: world has veins, hierarchical structure, and "world pose". Moreover, because the flow rate in the core space is slower than in reality, we can apply real-time control to certain objects, such as the keyboard (with an input speed of typically 1000Hz).
- **How do we better control the world?** Our answer: Just as controlling the human body requires skeletal conditions, controlling the world demands more. We create a core space of wave functions, just like—the world's shadow, the ocean's lower layer—modeling probability distributions and achieving virtual-real conversion.
- **Many top researchers intuitively feel that the potential of Rectified Flow has not been fully exploited.** Though RF is excellent work, but it lacks control mechanisms, has to submit to diffusion and autoregression. Our answer: Create core space to better utilize RF model's reverse operations, create wsg to achieve hundreds of conditional controls at pixel-level.
- **How do we achieve long-term consistency and overcome the "quadratic" dark cloud?** Our answer: Divide the model into two orthogonal parts: (1) reconstruction, time-space, order and existence; (2) generation, rendering, understanding and cognition. Handle these separately rather than forcing simultaneous attention. For instance, although creating a character video is quite challenging, if a 100-hour or infinite-time DWpose visualized video can be provided, then any popular pose2img model could theoretically achieve infinite-time rendering using the reference image. The waves are the skeleton of the world.
- **How to achieve variable resolution + variable frame rate + real-time?** As mentioned in last answer, we divide the orthogonal task goals into two models to handle. The existing models can both achieve good rendering (although most of them are <0.3 fps and cannot generate wsg). At this point, the pressure is on the wave space generation here. The wave space is an operation-style space and operates at 8 FPS, which greatly reduces the pressure on the models to maintain the world's operation. Then, by observing the core space + external rendering + LCoPE when training, it is naturally possible to restore to unlimited FPS, thus achieving variable resolution + variable unlimited frame rate. The rendering process can achieve real-time generation and understanding by using miniature RF models.

Apart from trying to solve the truly difficult problems, our contributions can be summarized as follows:

- We proposed <a style="color: green;">**World Snapshot**</a> (The [Definition](https://world-snapshot.github.io/doc/index.html?page=S5_blogs/00_world_model.md#definition) & [Implementation](https://github.com/World-Snapshot/WaveGen) of WSMs; [WSG](https://world-snapshot.github.io/doc/index.html?page=S2_standard/02_wsg_ver1.md), [WSV](https://world-snapshot.github.io/doc/index.html?page=S2_standard/03_wsv_ver1.md), [WSF](https://world-snapshot.github.io/doc/index.html?page=S2_standard/04_wsf_ver1.md); They mainly belonging to/related to Computer Vision).
- We proposed the <a style="color: green;">**Wave-based Generative Model**</a> ([WaveGen](https://github.com/World-Snapshot/WaveGen) and [ControlWave UI](https://github.com/World-Snapshot/ControlWave); it belongs to the generative method, similar to Diffusion, Flow, AR, GAN, etc). WaveGen also can be used for unifying reconstruction, and can be converted among the five reconstruction methods (GS, Point Cloud, NeRF, Mesh, and Wave).
- <a style="color: green;">**Improve the control mechanism of the wave model:**</a> wsg gives pixel-level control; the "virtual-real conversion mechanism + channel mutual condition" gives the model 2D instance-level control; and the wave space provides 3D instance-level control. Additionally, based on RF, WaveGen already has concept-level control.
- <a style="color: green;">**Some techniques:**</a> Sen Fang Initialization, LCoPE, the difference in flow rates inside/outside of world creates the possibility for causal learning, PLA Loss.
- <a style="color: green;">**Unifying 1500+ generation/understanding/reconstruction/estimation tasks in just one model**</a> at 30FPS. Create/Born 500+ additional small tasks newly. We unify Computer Vision, capture portions of NLP and Audio fields, creating the World Snapshot Models (WSMs) field, equivalent of LLMs' status in NLP (LLMs captures a portion of the CV and Audio fields as well, and can almost handle all NLP tasks).

<!--
- **What have we achieved?** Our answer: 
-->