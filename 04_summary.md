## Summary

In this article, we <a style="color: green;">**attempt to address the real challenges**</a> currently faced by Computer Vision:

- **How do we define and implement the ideal world model?** Our answer: (1) [9-point definitions](https://world-snapshot.github.io/doc/index.html?page=S5_blogs/00_world_model.md#definition); (2) [Wave Generative method](https://world-snapshot.github.io/doc/index.html?page=S3_get_start/01_wave_gen.md) (abbreviated as [WaveGen](https://github.com/World-Snapshot/WaveGen)); (3) [World Snapshot methodology](https://world-snapshot.github.io/doc/index.html?page=S2_standard/01_overview.md): ["Encoding all information onto boundaries + Conformal symmetry"](https://world-snapshot.github.io/doc/index.html?page=S2_standard/01_overview.md#supplement) — has a similar meaning to LLMs' "Predict next token" methodology. Predicting the next token can lead to the creation of LLM, and encoding information onto the surface of the world image can result in the creation of WSM.
- **How do we achieve true multimodal fusion and storage?** Our answer: Store maximum information within images themselves, achieving near 100% or theoretically exceeding [100% modal fusion degree](https://github.com/World-Snapshot/100-percent-multimodal-fusion)<sup class="footnote" data-note="If we label a 123to321 type modal transformation model that has fully utilized all its neurons as having a 100% multi-modal fusion degree, then the method of recovering multiple modalities from a single modality (1to123) is what we need to consider next. Ensuring that the temporal and spatial correspondences are consistent when reading the dataset can further enhance the potential. Therefore, WSG theoretically has the potential to surpass the upper limit of existing models (if we define the upper limit of existing models as 100%).">1</sup> (ie., in a nutshell, under the same data volume and training time, the ratio of achieving n% multimodal fusion degree to a certain parameter quantity, the more, the better. Or another form of this indicator). This is training the world itself.
- **How do we make AI understand physics?** Our answer: Transform the generative model's parameter space into an engine-like space. We [analyze error sources](https://world-snapshot.github.io/doc/index.html?page=S6_materials/02_guideline.md#how-do-we-successfully-create-physics-and-causality) and approach perfection through multiple angles—training strategies, high-dimensional spaces, and low-dimensional spaces, i.e., via least action principle + core space + WSG format, correspondingly.
- **How do we achieve causal generation?** Our answer: Recognize and make world divides into internal and external flow velocity differences (core space & pixel world). Humans control the slower internal world, thus achieving causal reality generation. To be more specific, our core space acts like a operating micro-causal space/engine: world has veins, hierarchical structure, and "world pose". Moreover, because the flow rate in the core space is slower than in reality, we can apply real-time control to certain objects, such as the keyboard (with an input speed of typically 1000Hz). Or, to be precise, it is the differences in FPS (frames per second) / the world snapshot interval (akin to [Planck time](https://en.wikipedia.org/wiki/Planck_units#Planck_time) but in Computer Vision<sup class="footnote" data-note="Our world might be a world snapshot model created by that higher-dimensional universe. The intervals between these wsg are indeed very similar to the Planck time. Of course, this is just a conjecture.">3</sup>) that allow us to modify the slower actionable space.
- **How do we better control the world?** Our answer: Just as controlling the human body requires skeletal conditions, controlling the world demands more. We create a core space of wave functions, just like—the world's shadow, the ocean's lower layer—modeling probability distributions and achieving virtual-real conversion.
- **Many top researchers intuitively feel that the potential of Rectified Flow has not been fully exploited.** Though RF is excellent work, but it lacks control mechanisms<sup class="footnote" data-note="RF is proud of its concept-level control. Although it is excellent, compared to the other two, it actually requires a control method that suits its characteristics.">4</sup>, has to submit to diffusion and autoregression. Our answer: Create core space to better utilize RF model's reverse operations, create wsg to achieve hundreds of conditional controls at pixel-level.
- **How to unify the generation and understanding:** This is actually not a problem. Most of the RF models during training have already been a unified model<sup class="footnote" data-note="After RGB understanding, it becomes a Gaussian, and the meaning is lost. In fact, a thing like a core space surface is needed to carry the understanding. More specifically, one is from text to generate the surface of the core space (for the generation task), and the other is from RGB to generate the surface of the core space (for the understanding/estimation task). Generation and understanding are actually a kind of task. The conversion between them requires avoiding overlapping with each other. The reverse operation of RGB actually involves obtaining useful information, but this useful information is then covered by a Gaussian distribution.">5</sup>, the only issue is that during the understanding process, there is no tensor with the same format as the world frame (such as RGB) to store the understanding information (The understood information is covered by a Gaussian distribution). This is just an elaboration of the previous point, and it shouldn't have been written. More detail at [Unified Law for Visual Tasks](https://github.com/orgs/World-Snapshot/discussions/1).
- **How do we achieve long-term consistency and overcome the "quadratic" dark cloud?** Our answer: Divide the model into two orthogonal parts<sup class="footnote" data-note="Only when you focus on one of the two types of tasks will you propose WaveXyy or XyyWSM.">6</sup>: (1) reconstruction, time-space, order and existence; (2) generation, rendering, understanding and cognition. Handle these separately rather than forcing simultaneous attention. For instance, although creating a character video is quite challenging, if a 100-hour or infinite-time DWpose visualized video can be provided, then any popular pose2img model could theoretically achieve infinite-time rendering using the reference image. The waves are the skeleton of the world.
- **How to achieve variable resolution + variable frame rate + real-time?** As mentioned in last answer, we divide the orthogonal task goals into two models to handle. The existing models can both achieve good rendering (although most of them are <0.3 fps and cannot generate wsg). At this point, the pressure is on the wave space generation here. The wave space is an operation-style space<sup class="footnote" data-note="For instance, even if humans move their arms, we can say that the frame rate is infinitely high, but the frequency of the operation is only 1 FPS - that is, the neurons only issue one command to move the fingers.">7</sup> and operates at 8 FPS, which greatly reduces the pressure on the models to maintain the world's operation. Then, by observing the core space + external rendering + LCoPE when training, it is naturally possible to restore to unlimited FPS, thus achieving variable resolution + variable unlimited frame rate. The rendering process can achieve real-time generation and understanding by using miniature RF models.

Apart from <a style="color: green;">**trying to solve the truly difficult challenges**</a><sup class="footnote" data-note="The above content can be summarized as the characteristics of our method: like variable FPS, variable resolution, causality, physics, consistency, training the world itself, predicting the future/past, synchronizing the real world with model's core space, and pixel-level control of the world distribution, unlimited duration, native 3D+t, etc.">8</sup>, our contributions can be summarized as follows:

- We proposed <a style="color: green;">**World Snapshot**</a> (The [Definition](https://world-snapshot.github.io/doc/index.html?page=S5_blogs/00_world_model.md#definition) & [Implementation](https://github.com/World-Snapshot/WaveGen) of WSMs; [WSG](https://world-snapshot.github.io/doc/index.html?page=S2_standard/02_wsg_ver1.md), [WSV](https://world-snapshot.github.io/doc/index.html?page=S2_standard/03_wsv_ver1.md), [WSF](https://world-snapshot.github.io/doc/index.html?page=S2_standard/04_wsf_ver1.md); They mainly belonging to/related to Computer Vision).
- We proposed the <a style="color: green;">**Wave-based Generative Method**</a> ([WaveGen](https://github.com/World-Snapshot/WaveGen); It actually belongs to the unified approach, in generative tasks, it similar to Diffusion, Flow, AR, GAN, etc). WaveGen also can be used for unifying reconstruction<sup class="footnote" data-note="At present, it is only implemented within a very limited and simple scope. After all, it is a new method and requires more time.">9</sup>, and can be converted among the seven reconstruction methods (GS<sup class="footnote" data-note="The world snapshot model trained by WaveGen contains shape and color information, but it does not support GS as closely as other methods. After all, fundamentally, waves and Gaussian volumes are both representations of shapes, and there is no other way to convert them that is more convenient.">10</sup>, Point Cloud, NeRF<sup class="footnote" data-note="In fact, we use WSF to simulate NeRF. The difference is that the model is separated from the scene data. The advantage is that it can store more scenes, while the disadvantage is that a model (RF pixels decoder) is required when output for each scene. Moreover, this feature is only a prototype and needs improvement.">11</sup>, Mesh, Voxel, SDF, and Wave).
- <a style="color: green;">**Improve the control mechanism of the wave model:**</a> WSG gives pixel-level control; the "virtual-real conversion mechanism + channel mutual condition" gives the model 2D instance-level control; and the wave space provides 3D-4D instance-level control. Additionally, based on RF, WaveGen already has concept-level control. We also develop a [ControlWave UI](https://github.com/World-Snapshot/ControlWave) for it.
- <a style="color: green;">**Some new techniques:**</a> [Sen Fang Initialization](https://github.com/World-Snapshot/Sen-Fang-Initialization); LCoPE; the difference in flow rates inside/outside of world creates the possibility for causal learning; [Unified Law for Visual Tasks](https://github.com/orgs/World-Snapshot/discussions/1); PLA Loss fuction; [StreamFlow](https://github.com/World-Snapshot/wave2pixel/tree/main/streamflow)<sup class="footnote" data-note="To speed up our decoder (based Rectified Flow), we developed a library to accelerate the Rectified Flow model, which can achieve a speedup of 300% to 600% and supports unlimited multi-GPU decoding. We wrote approximately 30,000 to 100,000 lines of code to improve this acceleration library.">12</sup>.
- <a style="color: green;">**Unifying 1500+ generation/understanding/reconstruction/estimation tasks in just one model**</a> at 30FPS. Create/Born 500+ additional small tasks newly. We unify Computer Vision, capture portions of NLP and Audio fields, creating the World Snapshot Models (WSMs) field, equivalent of LLMs' status in NLP (LLMs captures a portion of the CV and Audio fields as well, and can almost handle all NLP tasks).


<!--
- **What have we achieved?** Our answer: 
-->